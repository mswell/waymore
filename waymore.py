#!/usr/bin/env python
# Python 3
# waymore - by @Xnl-h4ck3r: Find way more from the Wayback Machine
# Full help here: https://github.com/xnl-h4ck3r/waymore/blob/main/README.md
# Good luck and good hunting! If you really love the tool (or any others), or they helped you find an awesome bounty, consider BUYING ME A COFFEE! (https://ko-fi.com/xnlh4ck3r) ☕ (I could use the caffeine!)

import requests
from requests.exceptions import ConnectionError
from requests.utils import quote
from requests.adapters import HTTPAdapter, Retry
import argparse
from signal import SIGINT, signal
import multiprocessing.dummy as mp
from termcolor import colored
from datetime import datetime
from pathlib import Path
import yaml
import os
import json
import re
import random
import sys
import math

# Global variables
linksFound = set()
linkMimes = set()
stopProgram = False
stopProgramCount = 0
stopSource = False
successCount = 0
failureCount = 0
fileCount = 0
totalResponses = 0
totalPages = 0
indexFile = None
inputIsDomainANDPath = False
subs = '*.'
path = ''
waymorePath = Path()
HTTP_ADAPTER = None
SPACER = ' ' * 70

# Source Provider URLs
WAYBACK_URL = 'https://web.archive.org/cdx/search/cdx?url={DOMAIN}&collapse={COLLAPSE}&fl=timestamp,original,mimetype,statuscode,digest'
CCRAWL_INDEX_URL = 'https://index.commoncrawl.org/collinfo.json'
ALIENVAULT_URL = 'https://otx.alienvault.com/api/v1/indicators/domain/{DOMAIN}/url_list?limit=500'
URLSCAN_URL = 'https://urlscan.io/api/v1/search/?q=domain:{DOMAIN}&size=10000'

# User Agents to use when making requests, chosen at random
USER_AGENT  = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246",
    "Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36 Edg/99.0.1150.36",
    "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko",
    "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0",
]

# The default maximum number of responses to download
DEFAULT_LIMIT = 5000

# The default timeout for archived responses to be retrieved in seconds
DEFAULT_TIMEOUT = 30

# Exclusions used to exclude responses we will try to get from web.archive.org
DEFAULT_FILTER_URL = '.css,.jpg,.jpeg,.png,.svg,.img,.gif,.mp4,.flv,.ogv,.webm,.webp,.mov,.mp3,.m4a,.m4p,.scss,.tif,.tiff,.ttf,.otf,.woff,.woff2,.bmp,.ico,.eot,.htc,.rtf,.swf,.image,/image,/img,/css,/wp-json,/wp-content,/wp-includes,/theme,/audio,/captcha,/font,node_modules,/jquery,/bootstrap'

# MIME Content-Type exclusions used to filter links and responses from web.archive.org through their API
DEFAULT_FILTER_MIME = 'text/css,image/jpeg,image/jpg,image/png,image/svg+xml,image/gif,image/tiff,image/webp,image/bmp,image/vnd,image/x-icon,image/vnd.microsoft.icon,font/ttf,font/woff,font/woff2,font/x-woff2,font/x-woff,font/otf,audio/mpeg,audio/wav,audio/webm,audio/aac,audio/ogg,audio/wav,audio/webm,video/mp4,video/mpeg,video/webm,video/ogg,video/mp2t,video/webm,video/x-msvideo,video/x-flv,application/font-woff,application/font-woff2,application/x-font-woff,application/x-font-woff2,application/vnd.ms-fontobject,application/font-sfnt,application/vnd.android.package-archive,binary/octet-stream,application/octet-stream,application/pdf,application/x-font-ttf,application/x-font-otf,video/webm,video/3gpp,application/font-ttf,audio/mp3,audio/x-wav,image/pjpeg,audio/basic'

# Response code exclusions we will use to filter links and responses from web.archive.org through their API
DEFAULT_FILTER_CODE = '404,301,302'

# Yaml config values
FILTER_URL = ''
FILTER_MIME = ''
FILTER_CODE = ''
URLSCAN_API_KEY = ''

# Define colours
class tc:
    NORMAL = '\x1b[39m'
    RED = '\x1b[31m'
    GREEN = '\x1b[32m'
    YELLOW = '\x1b[33m'
    MAGENTA = '\x1b[35m'
    CYAN = '\x1b[36m'

def showBanner():
    print()
    print(tc.RED+" _ _ _       _   _ "+tc.NORMAL+"____                    ")
    print(tc.RED+"| | | |_____| | | "+tc.NORMAL+"/    \  ___   ____ _____ ")
    print(tc.RED+"| | | (____ | | | "+tc.NORMAL+"| | | |/ _ \ / ___) ___ |")
    print(tc.RED+"| | | / ___ | |_| "+tc.NORMAL+"| | | | |_| | |   | |_| |")
    print(tc.RED+" \___/\_____|\__  "+tc.NORMAL+"|_|_|_|\___/| |   | ____/")
    print(tc.RED+"            (____/ "+tc.MAGENTA+"  by Xnl-h4ck3r "+tc.NORMAL+" \_____)")
    print()

def verbose():
    """
    Functions used when printing messages dependant on verbose option
    """
    return args.verbose

def handler(signal_received, frame):
    """
    This function is called if Ctrl-C is called by the user
    An attempt will be made to try and clean up properly
    """
    global stopSource, stopProgram, stopProgramCount

    if stopProgram:
        stopProgramCount = stopProgramCount + 1
        if stopProgramCount == 1:
            print(colored(">>> Please be patient... Trying to save data and end gracefully!",'red'),SPACER)
        elif stopProgramCount == 2:
            print(colored(">>> SERIOUSLY... YOU DON'T WANT YOUR DATA SAVED?!", 'red'),SPACER)
        elif stopProgramCount == 3:
            print(colored(">>> Patience isn't your strong suit eh? ¯\_(ツ)_/¯", 'red'),SPACER)
            sys.exit()
    else:
        stopProgram = True
        stopSource = True
        print(colored('>>> "Oh my God, they killed Kenny... and waymore!" - Kyle', "red"),SPACER)
        print(colored(">>> Attempting to rescue any data gathered so far...", "red"),SPACER)

def showOptions():
    """
    Show the chosen options and config settings
    """
    global inputIsDomainANDPath
                
    try:
        print(colored('Selected config and settings:', 'cyan'))
        
        if inputIsDomainANDPath:
            print(colored('-i: ' + args.input, 'magenta'), 'The target URL to download responses for.')
        else: # input is a domain
            print(colored('-i: ' + args.input, 'magenta'), 'The target domain to find links for.')
        
        if args.mode == 'U':
            print(colored('-mode: ' + args.mode, 'magenta'), 'Only URLs will be retrieved for the input.')
        elif args.mode == 'R':
            print(colored('-mode: ' + args.mode, 'magenta'), 'Only Responses will be downloaded for the input.')
        elif args.mode == 'B':
            print(colored('-mode: ' + args.mode, 'magenta'), 'URLs will be retrieved AND Responses will be downloaded for the input.')
            
        if not inputIsDomainANDPath:
            if args.no_subs:
                print(colored('-n: ' +str(args.no_subs), 'magenta'), 'Sub domains are excluded in the search.')
            else:
                print(colored('-n: ' +str(args.no_subs), 'magenta'), 'Sub domains are included in the search.')
                
            print(colored('-xcc: ' +str(args.xcc), 'magenta'), 'Whether to exclude checks to commoncrawl.org. Searching all their index collections can take a while, and it may not return any extra URLs that weren\'t already found on archive.org')
            if not args.xcc:
                if args.lcc == 0:
                    print(colored('-lcc: ' +str(args.lcc), 'magenta'), 'Search ALL Common Crawl index collections.')
                else:
                    print(colored('-lcc: ' +str(args.lcc), 'magenta'), 'The number of latest Common Crawl index collections to be searched.')
            print(colored('-xav: ' +str(args.xav), 'magenta'), 'Whether to exclude checks to alienvault.com. Searching all their pages can take a while, and it may not return any extra URLs that weren\'t already found on archive.org')
            print(colored('-xus: ' +str(args.xus), 'magenta'), 'Whether to exclude checks to urlscan.io. Searching all their pages can take a while, and it may not return any extra URLs that weren\'t already found on archive.org')
            if URLSCAN_API_KEY == '':
                print(colored('URLScan API Key:', 'magenta'), '{none} - You can get a FREE or paid API Key at https://urlscan.io/user/signup which will let you get more bac, and quicker.')
            else:
                print(colored('URLScan API Key:', 'magenta'), URLSCAN_API_KEY)
            
        if args.mode in ['R','B']:
            if args.limit == 0:
                print(colored('-l: ' +str(args.limit), 'magenta'), 'Save ALL responses found.')
            else:
                if args.limit > 0:
                    print(colored('-l: ' +str(args.limit), 'magenta'), 'Only save the FIRST ' + str(args.limit) + ' responses found.')
                else:
                    print(colored('-l: ' +str(args.limit), 'magenta'), 'Only save the LAST ' + str(abs(args.limit)) + ' responses found.')
                    
            if args.from_date is not None:
                print(colored('-from: ' +str(args.from_date), 'magenta'), 'The date/time to get responses from.')
            if args.to_date is not None:
                print(colored('-to: ' +str(args.to_date), 'magenta'), 'The date/time to get responses up to.')
            
            if args.capture_interval == 'h': 
                print(colored('-ci: ' +args.capture_interval, 'magenta'), 'Get at most 1 archived response per hour from archive.org')
            elif args.capture_interval == 'd':
                print(colored('-ci: ' +args.capture_interval, 'magenta'), 'Get at most 1 archived response per day from archive.org')
            elif args.capture_interval == 'm':
                print(colored('-ci: ' +args.capture_interval, 'magenta'), 'Get at most 1 archived response per month from archive.org')
            elif args.capture_interval == 'none':
                print(colored('-ci: ' +args.capture_interval, 'magenta'), 'There will not be any filtering based on the capture interval.')
                    
            if args.url_filename:
                print(colored('-url-filename: ' +str(args.url_filename), 'magenta'), 'The filenames of downloaded responses wil be set to the URL rather than the hash value of the response.')
            
        print(colored('-f: ' +str(args.filter_responses_only), 'magenta'), 'If True, the initial links from wayback machine will not be filtered, only the responses that are downloaded will be filtered. It maybe useful to still see all available paths even if you don\'t want to check the file for content.')
        print(colored('MIME Type exclusions:', 'magenta'), FILTER_MIME)
        print(colored('Response Code exclusions:', 'magenta'), FILTER_CODE)      
        print(colored('Response URL exclusions:', 'magenta'), FILTER_URL)  
        
        if args.regex_after is not None:
            print(
                colored("-ra: " + args.regex_after, "magenta"),
                "RegEx for filtering purposes against found links from archive.org AND responses downloaded. Only positive matches will be output.",
            )
        if args.mode in ['R','B']:
            print(colored('-t: ' + str(args.timeout), 'magenta'),'The number of seconds to wait for a an archived response.')
        if args.mode in ['R','B'] or (args.mode == 'U' and not args.xcc):
            print(colored('-p: ' + str(args.processes), 'magenta'), 'The number of parallel requests made.')
        print(colored('-r: ' + str(args.retries), 'magenta'), 'The number of retries for requests that get connection error or rate limited.')
        
        print()

    except Exception as e:
        print(colored('ERROR showOptions: ' + str(e), 'red'))

def getConfig():
    """
    Try to get the values from the config file, otherwise use the defaults
    """
    global FILTER_CODE, FILTER_MIME, FILTER_URL, URLSCAN_API_KEY, subs, path, waymorePath, inputIsDomainANDPath, HTTP_ADAPTER
    try:
        
        # If the input doesn't have a / then assume it is a domain rather than a domain AND path
        if str(args.input).find('/') < 0:
            path = '/*'
            inputIsDomainANDPath = False
        else:
            # If the input is a URL then -mode will have to be R (Responses only)
            inputIsDomainANDPath = True
            args.mode = 'R'
            
        # If the -no-subs argument was passed, OR the input is a path, don't include subs
        if args.no_subs or inputIsDomainANDPath:
            subs = ''
        
        # Set up an HTTPAdaptor for retry strategy when making requests
        try:
            retry= Retry(
                total=args.retries,
                backoff_factor=1,
                status_forcelist=[429, 500, 502, 503, 504],
                raise_on_status=False,
                respect_retry_after_header=False
            )
            HTTP_ADAPTER = HTTPAdapter(max_retries=retry)
        except Exception as e:
            print(colored('ERROR getConfig 2: ' + str(e), 'red'))
            
        # Try to get the config file values
        try:        
            waymorePath.resolve()
            if waymorePath == '':
                configPath = 'config.yml'
            else:
                configPath = waymorePath / 'config.yml'
            config = yaml.safe_load(open(configPath))
        
            try:
                FILTER_URL = config.get('FILTER_URL')
                if str(FILTER_URL) == 'None':
                    FILTER_URL = ''
            except Exception as e:
                print(colored('Unable to read "FILTER_URL" from config.yml; defaults set', 'red'))
                FILTER_URL = DEFAULT_FILTER_URL

            try:
                FILTER_MIME = config.get('FILTER_MIME')
                if str(FILTER_MIME) == 'None':
                    FILTER_MIME = ''
            except Exception as e:
                print(colored('Unable to read "FILTER_MIME" from config.yml; defaults set', 'red'))
                FILTER_MIME = DEFAULT_FILTER_MIME
            
            try:
                FILTER_CODE = config.get('FILTER_CODE')
                if str(FILTER_CODE) == 'None':
                    FILTER_CODE = ''
            except Exception as e:
                print(colored('Unable to read "FILTER_CODE" from config.yml; defaults set', 'red'))
                FILTER_CODE = DEFAULT_FILTER_CODE
                
            try:
                URLSCAN_API_KEY = config.get('URLSCAN_API_KEY')
                if str(URLSCAN_API_KEY) == 'None':
                    URLSCAN_API_KEY = ''
            except Exception as e:
                print(colored('Unable to read "URLSCAN_API_KEY" from config.yml; defaults set', 'red'))
                URLSCAN_API_KEY = ''
                
        except:
            print(colored('WARNING: Cannot find config.yml, so using default values\n', 'yellow'))
            FILTER_URL = DEFAULT_FILTER_URL
            FILTER_MIME = DEFAULT_FILTER_MIME
            FILTER_CODE = DEFAULT_FILTER_CODE
            URLSCAN_API_KEY = ''
            
    except Exception as e:
        print(colored('ERROR getConfig 1: ' + str(e), 'red'))

# Print iterations progress - copied from https://stackoverflow.com/questions/3173320/text-progress-bar-in-terminal-with-block-characters?noredirect=1&lq=1
def printProgressBar(
    iteration,
    total,
    prefix="",
    suffix="",
    decimals=1,
    length=100,
    fill="█",
    printEnd="\r",
):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        length      - Optional  : character length of bar (Int)
        fill        - Optional  : bar fill character (Str)
        printEnd    - Optional  : end character (e.g. "\r", "\r\n") (Str)
    """
    try:
        percent = ("{0:." + str(decimals) + "f}").format(
            100 * (iteration / float(total))
        )
        filledLength = int(length * iteration // total)
        bar = fill * filledLength + "-" * (length - filledLength)
        print(colored(f"\r{prefix} |{bar}| {percent}% {suffix}", "green"), end=printEnd)
        # Print New Line on Complete
        if iteration == total:
            print()
    except Exception as e:
        if verbose():
            print(colored("ERROR printProgressBar: " + str(e), "red"))

def filehash(text):
    """
    Generate a hash value for the passed string. This is used for the file name of a downloaded archived response
    """
    hash=0
    for ch in text:
        hash = (hash*281 ^ ord(ch)*997) & 0xFFFFFFFFFFF
    return str(hash)    

class WayBackException(Exception):
    """
    A custom exception to raise if archive.org respond with specific text in the response that indicate there is a problem on their side
    """
    def __init__(self):
         message = f"WayBackException"
         super().__init__(message)

def fixArchiveOrgUrl(url):
    """
    Sometimes archive.org returns a URL that has %0A at the end followed by other characters. If you try to reach the archive URL with that it will fail, but remove from the %0A (newline) onwards and it succeeds, so it doesn't seem intentionally included. In this case, strip anything from %0A onwards from the URL
    """
    newline = url.find('%0A')
    if newline > 0:
        url = url[0:newline]
    else:
        newline = url.find('%0a')
        if newline > 0:
            url = url[0:newline]
    return url
                
def processArchiveUrl(url):
    """
    Get the passed web archive response
    """
    global stopProgram, successCount, failureCount, fileCount, waymorePath, totalResponses, indexFile
    try:
        if not stopProgram:
            
            archiveUrl = 'https://web.archive.org/web/' + fixArchiveOrgUrl(url)
            hashValue = ''
        
            # Make a request to the web archive
            try:
                try:
                    # Choose a random user agent string to use for any requests
                    userAgent = random.choice(USER_AGENT)

                    session = requests.Session()
                    session.mount('https://', HTTP_ADAPTER)
                    session.mount('http://', HTTP_ADAPTER)
                    resp = session.get(url = archiveUrl, headers={"User-Agent":userAgent}, allow_redirects = False)
                    archiveHtml = str(resp.text)
                    
                    # Only create a file if there is a response
                    if len(archiveHtml) != 0:
                        
                        # Add the URL as a comment at the start of the response
                        if args.url_filename:
                            archiveHtml = '/* Original URL: ' + archiveUrl + ' */\n' + archiveHtml
                        
                        # Remove all web archive references in the response
                        archiveHtml = re.sub(r'\<head\>.*\<\!-- End Wayback Rewrite JS Include --\>','<head>',archiveHtml,1,flags=re.DOTALL|re.IGNORECASE)
                        archiveHtml = re.sub(r'\<script src=\"\/\/archive\.org.*\<\!-- End Wayback Rewrite JS Include --\>','',archiveHtml,1,flags=re.DOTALL|re.IGNORECASE)
                        archiveHtml = re.sub(r'\<\!-- BEGIN WAYBACK TOOLBAR INSERT --\>.*\<\!-- END WAYBACK TOOLBAR INSERT --\>','',archiveHtml,1,flags=re.DOTALL|re.IGNORECASE)
                        archiveHtml = re.sub(r'FILE ARCHIVED ON.*108\(a\)\(3\)\)\.','',archiveHtml,1,flags=re.DOTALL|re.IGNORECASE)
                        archiveHtml = re.sub(r'(\<\!--|\/\*)\nplayback timings.*(--\>|\*\/)','',archiveHtml,1,flags=re.DOTALL|re.IGNORECASE)
                        archiveHtml = re.sub(r'((https:)?\/\/web\.archive\.org)?\/web\/[0-9]{14}([A-Za-z]{2}\_)?\/','',archiveHtml,flags=re.IGNORECASE)
                        archiveHtml = re.sub(r'((https:)?\\\/\\\/web\.archive\.org)?\\\/web\\\/[0-9]{14}([A-Za-z]{2}\_)?\\\/','',archiveHtml,flags=re.IGNORECASE)
                        archiveHtml = re.sub(r'((https:)?%2F%2Fweb\.archive\.org)?%2Fweb%2F[0-9]{14}([A-Za-z]{2}\_)?%2F','',archiveHtml,flags=re.IGNORECASE)
                        archiveHtml = re.sub(r'((https:)?\\u002F\\u002Fweb\.archive\.org)?\\u002Fweb\\u002F[0-9]{14}([A-Za-z]{2}\_)?\\u002F','',archiveHtml,flags=re.IGNORECASE)
                       
                        # If there is a specific Wayback error in the response, raise an exception 
                        if archiveHtml.lower().find('wayback machine has not archived that url') > 0 or archiveHtml.lower().find('snapshot cannot be displayed due to an internal error') > 0:
                            raise WayBackException
                        
                        # Create file name based on url or hash value of the response, depending on selection. Ensure the file name isn't over 255 characters 
                        if args.url_filename:
                            fileName = url.replace('/','-').replace(':','')
                        else:
                            hashValue = filehash(archiveHtml)
                            fileName = hashValue
                        filePath = (
                                waymorePath
                                / 'results'
                                / str(args.input).replace('/','-')
                                / f'{fileName[0:250]}.xnl'
                        )

                        # Write the file
                        try:
                            responseFile = open(filePath, 'w', encoding='utf8')
                            responseFile.write(archiveHtml)
                            responseFile.close()
                            fileCount = fileCount + 1
                        except Exception as e:
                            print(colored('[ ERR ] Failed to write file ' + filePath + ': '+ str(e), 'red'))
                            
                        # Write the hash value and URL to the index file
                        if not args.url_filename:
                            try:
                                timestamp = str(datetime.now())
                                indexFile.write(hashValue+','+archiveUrl+' ,'+timestamp+'\n')
                            
                            except Exception as e:
                                print(colored('[ ERR ] Failed to write to index.txt for "' + archiveUrl + '": '+ str(e), 'red'))

                        # FOR DEBUGGING PURPOSES
                        try:
                            if archiveHtml.find('archive.org') > 0 and os.environ.get('USER') == 'xnl':
                                print(colored('"' + hashValue + '.xnl" CONTAINS ARCHIVE.ORG - CHECK ITS A VALID REFERENCE', 'yellow'),SPACER)
                        except:
                            pass
                            
                    successCount = successCount + 1
                
                except WayBackException as wbe:
                    failureCount = failureCount + 1
                    if verbose():
                        print(colored('[ ERR ] archive.org returned a problem for "' + archiveUrl + '"', 'red'))
                except ConnectionError as ce:
                    failureCount = failureCount + 1
                    if verbose():
                        print(colored('[ ERR ] archive.org connection error for "' + archiveUrl + '"', 'red'))
                except Exception as e:
                    failureCount = failureCount + 1
                    if verbose():
                        try:
                            print(colored('[ ' + str(resp.status_code) +' ] Failed to get response for "' + archiveUrl + '"', 'red'))
                        except:
                            print(colored('[ ERR ] Failed to get response for "' + archiveUrl + '": '+ str(e), 'red'))
                
                # Show progress bar
                fillTest = (successCount + failureCount) % 2
                fillChar = "o"
                if fillTest == 0:
                    fillChar = "O"
                printProgressBar(
                    successCount + failureCount,
                    totalResponses,
                    prefix="Downloading " + str(totalResponses) + " responses: ",
                    suffix="Complete  ",
                    length=50,
                    fill=fillChar
                )
                    
            except Exception as e:
                if verbose():
                    print(colored('Error for "'+url+'": ' + str(e), 'red'))
        else:
            os.kill(os.getpid(),SIGINT)
            
    except Exception as e:
        print(colored('ERROR processArchiveUrl 1:  ' + str(e), 'red'))

def processURLOutput():
    """
    Show results of the URL output, i.e. getting URLs from archive.org and commoncrawl.org and write results to file
    """
    global linksFound, subs, path

    try:
            
        linkCount = len(linksFound)
        print(colored('Links found for ' + subs + args.input + ':', 'cyan'), str(linkCount) + ' 🤘'+SPACER+'\n')
        
        # Create 'results' and domain directory if needed
        createDirs()
        
        try:
            # Open the output file
            outFile = open(
                waymorePath
                / 'results'
                / str(args.input).replace('/','-')
                / 'waymore.txt',
                'w',
            )
        except Exception as e:
            if verbose():
                print(colored("ERROR processURLOutput 2: " + str(e), "red"))
                sys.exit()

        # Go through all links, and output what was found
        # If the -ra --regex-after was passed then only output if it matches
        outputCount = 0
        for link in linksFound:
            try:
                if args.regex_after is None or re.search(args.regex_after, link, flags=re.IGNORECASE):
                    # Don't write it if the link does not contain the requested domain (this can sometimes happen)
                    if link.find(args.input) >= 0:
                        outFile.write(link + "\n")
                        outputCount = outputCount + 1
            except Exception as e:
                if verbose():
                    print(colored("ERROR processURLOutput 3: " + str(e), "red"))

        # If there are less links output because of filters, show the new total
        if args.regex_after is not None and linkCount > 0 and outputCount < linkCount:
            print(colored('Links found after applying filter "' + args.regex_after + '":','cyan'), str(outputCount) + ' 🤘\n')
        
        # Close the output file
        try:
            outFile.close()
        except Exception as e:
            if verbose():
                print(colored("ERROR processURLOutput 4: " + str(e), "red"))

        if verbose():
            if outputCount == 0:
                print(colored('No links were found so nothing written to file.\n', 'cyan'))
            else:   
                print(
                    colored('Links successfully written to file', 'cyan'), 
                    waymorePath
                    / 'results'
                    / str(args.input).replace('/','-')
                    / 'waymore.txt'
                )

    except Exception as e:
        if verbose():
            print(colored("ERROR processURLOutput 1: " + str(e), "red"))

def processResponsesOutput():
    """
    Show results of the archive responses saved
    """
    global successCount, failureCount, subs, fileCount
    try:
        if failureCount > 0:
            print(colored('\nResponses saved for ' + subs + args.input + ':', 'cyan'), str(fileCount) +' (' +str(successCount-fileCount) + ' empty responses) 🤘', colored('(' + str(failureCount) + ' failed)\n','red'))
        else:
            print(colored('\nResponses saved for ' + subs + args.input + ':', 'cyan'), str(fileCount) +' (' +str(successCount-fileCount) + ' empty responses) 🤘\n')
    except Exception as e:
        if verbose():
            print(colored("ERROR processResponsesOutput 1: " + str(e), "red"))    

def validateArgProcesses(x):
    """
    Validate the -p / --processes argument 
    Only allow values between 1 and 5 inclusive
    """
    x = int(x) 
    if x < 1 or x > 5:
        raise argparse.ArgumentTypeError('The number of processes must be between 1 and 5. Be kind to archive.org and commoncrawl.org! :)')     
    return x

def validateArgInput(x):
    """
    Validate the -i / --input argument.
    Ensure it is a domain only, or a URL, but with no schema or query parameters or fragment
    """
    # Check if input seems to be valid domain or URL
    match = re.search(r"^((?!-))(xn--)?([a-z0-9][a-z0-9\-\_]{0,61}[a-z0-9]{0,1}\.)+(xn--)?([a-z0-9\-]{1,61}|[a-z0-9\-]{1,30}\.[a-z]{2,})(/[^\n|?#]*)?$", x)
    if match is None:
        raise argparse.ArgumentTypeError('Pass a domain only (with no schema) to search for all links, or pass a domain and path (with no schema) to just get archived responses for that URL. Do not pass a query string or fragment.')
    return x

def processAlienVaultPage(url):
    """
    Get URLs from a specific page of otx.alienvault.org API for the input domain
    """
    global totalPages, linkMimes, linksFound, stopSource
    try:
        if not stopSource:
            try:             
                # Choose a random user agent string to use for any requests
                userAgent = random.choice(USER_AGENT)
                page = url.split('page=')[1]
                session = requests.Session()
                session.mount('https://', HTTP_ADAPTER)
                session.mount('http://', HTTP_ADAPTER)
                resp = session.get(url, headers={"User-Agent":userAgent})  
            except ConnectionError as ce:
                print(colored('[ ERR ] alienvault.org connection error for page ' + page, 'red'),SPACER)
                resp = None
                return
            except Exception as e:
                print(colored('[ ERR ] Error getting response for page ' + page + ' - ' + str(e),'red'),SPACER)
                resp = None
                return
            finally:
                try:
                    if resp is not None:
                        # If a status other of 429, then stop processing Alien Vault
                        if resp.status_code == 429:
                            print(colored('[ 429 ] Alien Vault rate limit reached, so stopping. Links that have already been retrieved will be saved.','red'),SPACER)
                            stopSource = True
                            return
                        # If the response from alienvault.com is empty then skip
                        if resp.text == '' and totalPages == 0:
                            if verbose():
                                print(colored('[ ERR ] '+url+' gave an empty response.','red'),SPACER)
                            return
                        # If a status other than 200, then stop
                        if resp.status_code != 200:
                            if verbose():
                                print(colored('[ '+str(resp.status_code)+' ] Error for '+url,'red'),SPACER)
                            return
                except:
                    pass
            
            # Get the JSON response
            jsonResp = json.loads(resp.text.strip())

            # Go through each URL in the list
            for urlSection in jsonResp['url_list']:
                # Get the URL
                try:
                    foundUrl = urlSection['url']
                except:
                    foundUrl = ''

                # If a URL was found
                if foundUrl != '':    
                    # If filters are not required and subs are wanted then just add the URL to the list
                    if args.filter_responses_only and not args.no_subs:
                        linksFound.add(foundUrl)
                    else:
                        addLink = True
                        
                        # If the user requested -n / --no-subs then we don't want to add it if it has a sub domain (www. will not be classed as a sub domain)
                        if args.no_subs:
                            match = re.search(r'\:\/\/(www\.)?'+re.escape(args.input), foundUrl, flags=re.IGNORECASE)
                            if match is None:
                                addLink = False
                        
                        # If the user didn't requested -f / --filter-responses-only then check http code
                        # Note we can't check MIME filter because it is not returned by Alien Vault API
                        if addLink and not args.filter_responses_only: 
                            # Get the HTTP code
                            try:
                                httpCode = str(urlSection['httpcode'])
                            except:
                                httpCode = ''
                            
                            # If we have a HTTP Code, compare against the Code exclusions
                            if httpCode != '':
                                match = re.search(r'('+FILTER_CODE.replace(',','|')+')', httpCode, flags=re.IGNORECASE)
                                if match is not None:
                                    addLink = False
                            
                            # Check the URL exclusions
                            if addLink:
                                match = re.search(r'('+re.escape(FILTER_URL).replace(',','|')+')', foundUrl, flags=re.IGNORECASE)
                                if match is not None:
                                    addLink = False            
                                    
                        # Add link if it passed filters        
                        if addLink:
                            linksFound.add(foundUrl)
        else:
            pass                    
    except Exception as e:
        if verbose():
            print(colored("ERROR processLAlienVaultPage 1: " + str(e), "red"))
    
def getAlienVaultUrls():
    """
    Get URLs from the Alien Vault OTX, otx.alienvault.com
    """
    global linksFound, waymorePath, subs, path, stopProgram, totalPages, stopSource
    
    # Write the file of URL's for the passed domain/URL
    try:
        stopSource = False
        originalLinkCount = len(linksFound)
        
        url = ALIENVAULT_URL.replace('{DOMAIN}',quote(args.input))+'&page='
        
        # Get the number of pages (i.e. separate requests) that are going to be made to alienvault.com
        totalPages = 0
        try:
            print(colored('\rGetting the number of alienvault.com pages to search...','cyan'), end='\r')
            # Choose a random user agent string to use for any requests
            userAgent = random.choice(USER_AGENT)
            session = requests.Session()
            session.mount('https://', HTTP_ADAPTER)
            session.mount('http://', HTTP_ADAPTER)
            resp = session.get(url+'&showNumPages=True', headers={"User-Agent":userAgent}) 
        except Exception as e:
            print(colored('[ ERR ] unable to get links from alienvault.com: ' + str(e), 'red'),SPACER)
            return
        
        # If the rate limit was reached end now
        if resp.status_code == 429:
            print(colored('[ 429 ] Alien Vault rate limit reached so unable to get links.','red'),SPACER)
            return
        
        if verbose():
            print(colored('The Alien Vault URL requested to get links:','magenta'), url+'\n')
        
        # Carry on if something was found
        if resp.text.lower().find('"error": "') < 0:

            # Get the JSON response
            jsonResp = json.loads(resp.text.strip())
            
            # Try to get the number of results
            totalUrls = jsonResp['full_size']
            
            # If there are results, carry on
            if  totalUrls > 0:
                
                # Get total pages
                totalPages = math.ceil(totalUrls / 500)

                # if the page number was found then display it, but otherwise we will just try to increment until we have everything       
                print(colored('\rGetting links from ' + str(totalPages) + ' alienvault.com API requests (this can take a while for some domains)...','cyan'), end='\r')

                # Get a list of all the page URLs we need to visit
                pages = set()
                for page in range(1, totalPages + 1):
                    pages.add(url+str(page))

                # Process the URLs from alien vault 
                if not stopProgram:
                    p = mp.Pool(args.processes)
                    p.map(processAlienVaultPage, pages)
                    p.close()
                    p.join()
        else:
            if verbose():
                print(colored('[ ERR ] An error was returned in the alienvault.com response.', 'red'),SPACER+'\n')
                
        linkCount = len(linksFound) - originalLinkCount
        print(colored('Extra links found on alienvault.com:', 'cyan'), str(linkCount) + SPACER+'\n')
        
    except Exception as e:
        print(colored('ERROR getAlienVaultUrls 1: ' + str(e), 'red'))

def processURLScanUrl(url, httpCode, mimeType):
    """
    Process a specific URL from urlscan.io to determine whether to save the link
    """
    addLink = True
    
    try:      
        # If filters are required then test them
        if not args.filter_responses_only:
            
            # If the user requested -n / --no-subs then we don't want to add it if it has a sub domain (www. will not be classed as a sub domain)
            if args.no_subs:
                match = re.search(r'^[A-za-z]*\:\/\/(www\.)?'+re.escape(args.input), url, flags=re.IGNORECASE)
                if match is None:
                    addLink = False
        
            # If the user didn't requested -f / --filter-responses-only then check http code
            # Note we can't check MIME filter because it is not returned by URLScan API
            if addLink and not args.filter_responses_only: 
                
                # If we have a HTTP Code, compare against the Code exclusions
                if httpCode != '':
                    match = re.search(r'('+FILTER_CODE.replace(',','|')+')', httpCode, flags=re.IGNORECASE)
                    if match is not None:
                        addLink = False
                
                # Check the URL exclusions
                if addLink:
                    match = re.search(r'('+re.escape(FILTER_URL).replace(',','|')+')', url, flags=re.IGNORECASE)
                    if match is not None:
                        addLink = False            
                
                # Check the MIME exclusions
                if mimeType != '':
                    match = re.search(r'('+re.escape(FILTER_MIME).replace(',','|')+')', mimeType, flags=re.IGNORECASE)
                    if match is not None:
                        addLink = False
                    else:
                        linkMimes.add(mimeType)            
                    
        # Add link if it passed filters        
        if addLink:
            # Just get the domain of the url found by removing anything after ? and then from 3rd / 
            domainOnly = url.split('?',1)[0]
            domainOnly = '/'.join(domainOnly.split('/',3)[:3])

            # URLScan can return URLs that aren't for the domain passed so we need to check for those and not process them
            # Check the URL
            match = re.search(r'^[A-za-z]*\:\/.*(\/|\.)'+re.escape(args.input)+'$', domainOnly, flags=re.IGNORECASE)
            if match is not None:
                linksFound.add(url)  
            
    except Exception as e:
        print(colored('ERROR processURLScanUrl 1: ' + str(e), 'red'))
        
def getURLScanUrls():
    """
    Get URLs from the URLSCan API, urlscan.io
    """
    global URLSCAN_API_KEY, linksFound, linkMimes, waymorePath, subs, stopProgram, stopSource
    
    # Write the file of URL's for the passed domain/URL
    try:
        stopSource = False
        linkMimes = set()
        originalLinkCount = len(linksFound)
        
        url = URLSCAN_URL.replace('{DOMAIN}',quote(args.input))
        
        if verbose():
            print(colored('The URLScan URL requested to get links:','magenta'), url+'\n')
                   
        print(colored('\rGetting links from urlscan.io API (this can take a while for some domains)...','cyan'), end='\r')
       
        # Get the first page from urlscan.io
        try:
            # Choose a random user agent string to use for any requests
            userAgent = random.choice(USER_AGENT)
            session = requests.Session()
            session.mount('https://', HTTP_ADAPTER)
            session.mount('http://', HTTP_ADAPTER)
             # Pass the API-Key header too. This can change the max endpoints per page, depending on URLScan subscription
            resp = session.get(url, headers={'User-Agent':userAgent, 'API-Key':URLSCAN_API_KEY})
        except Exception as e:
            print(colored('[ ERR ] unable to get links from urlscan.io: ' + str(e), 'red'),SPACER)
            return
        
        # If the rate limit was reached or if a 401 (which likely means the API key isn't valid), try without API key
        if resp.status_code in (401,429): 
            if URLSCAN_API_KEY != '':
                try:
                    if resp.status_code == 429:
                        print(colored('[ 429 ] URLScan rate limit reached so trying without API Key...','red'),SPACER)
                    else:
                        print(colored('The URLScan API Key is invalid so trying without API Key...','red'),SPACER)
                    # Set key to blank for further requests
                    URLSCAN_API_KEY = ''
                    resp = requests.get(url, headers={'User-Agent':userAgent}) 
                except Exception as e:
                    print(colored('[ ERR ] unable to get links from urlscan.io: ' + str(e), 'red'),SPACER)
                    return
                
                # If the rate limit was reached end now
                if resp.status_code == 429:
                    print(colored('[ 429 ] URLScan rate limit reached without API Key so unable to get links.','red'),SPACER)
                    return
            else:
                print(colored('[ 429 ] URLScan rate limit reached so unable to get links.','red'),SPACER)
                return
        elif resp.status_code != 200:
            print(colored('[ ' + str(resp.status_code) + ' ] unable to get links from urlscan.io','red'),SPACER)
            return
        
        # Get the JSON response
        jsonResp = json.loads(resp.text.strip())
  
        # Get the number of results
        totalUrls = jsonResp['total']
        
        # Carry on if something was found
        if int(totalUrls) > 0:
            
            while not stopSource:
                
                searchAfter = ''
                
                # Go through each URL in the list
                for urlSection in jsonResp['results']:
                    
                    # Get the URL
                    try:
                        foundUrl = urlSection['page']['url']
                    except:
                        foundUrl = ''
                    
                    # Also get the "ptr" field which can also be a url we want
                    try:
                        pointer = urlSection['page']['ptr']
                        if not pointer.startswith('http'):
                            pointer = 'http://' + pointer
                    except:
                        pointer = ''

                    # Get the sort value used for the search_after parameter to get to the next page later
                    try:
                        sort = urlSection['sort']
                    except:
                        sort = ''
                    searchAfter = '&search_after='+str(sort[0])+','+str(sort[1])
                    
                    # Get the HTTP code
                    try:
                        httpCode = str(urlSection['page']['status'])
                    except:
                        httpCode = ''
                    
                    # Get the MIME type
                    try:
                        mimeType = urlSection['page']['mimeType']
                    except:
                        mimeType = ''
                    
                    # If a URL was found the process it
                    if foundUrl != '':    
                        processURLScanUrl(foundUrl, httpCode, mimeType)
                    
                    # If a pointer was found the process it
                    if pointer != '':    
                        processURLScanUrl(pointer, httpCode, mimeType)
                
                # If we have the field value to go to the next page...
                if searchAfter != '':
                
                    # Get the next page from urlscan.io
                    try:
                        # Choose a random user agent string to use for any requests
                        userAgent = random.choice(USER_AGENT)
                        session = requests.Session()
                        session.mount('https://', HTTP_ADAPTER)
                        session.mount('http://', HTTP_ADAPTER)
                        # Pass the API-Key header too. This can change the max endpoints per page, depending on URLScan subscription
                        resp = session.get(url+searchAfter, headers={'User-Agent':userAgent, 'API-Key':URLSCAN_API_KEY}) 
                    except Exception as e:
                        print(colored('[ ERR ] unable to get links from urlscan.io: ' + str(e), 'red'),SPACER)
                        pass
                    
                    # If the rate limit was reached end now
                    if resp.status_code == 429:
                        print(colored('[ 429 ] URLScan rate limit reached, so stopping. Links that have already been retrieved will be saved.','red'),SPACER)
                        stopSource = True
                        pass
                    elif resp.status_code != 200:
                        print(colored('[ ' + str(resp.status_code) + ' ] unable to get links from urlscan.io','red'),SPACER)
                        stopSource = True
                        pass

                    if not stopSource:
                        # Get the JSON response
                        jsonResp = json.loads(resp.text.strip())
                        
                        # If there are no more results then stop
                        if len(jsonResp['results']) == 0:
                            stopSource = True
        
        # Show the MIME types found (in case user wants to exclude more)
        if verbose() and len(linkMimes) > 0:
            linkMimes.discard('warc/revisit')
            print(colored('MIME types found:','magenta'), str(linkMimes) + SPACER +'\n')
                    
        linkCount = len(linksFound) - originalLinkCount
        print(colored('Extra links found on urlscan.io:', 'cyan'), str(linkCount) + SPACER+'\n')
        
    except Exception as e:
        print(colored('ERROR getURLScanUrls 1: ' + str(e), 'red'))
        
def processWayBackPage(url):
    """
    Get URLs from a specific page of archive.org CDX API for the input domain
    """
    global totalPages, linkMimes, linksFound, stopSource
    try:
        if not stopSource:
            try:             
                # Choose a random user agent string to use for any requests
                userAgent = random.choice(USER_AGENT)
                page = url.split('page=')[1]
                session = requests.Session()
                session.mount('https://', HTTP_ADAPTER)
                session.mount('http://', HTTP_ADAPTER)
                resp = session.get(url, headers={"User-Agent":userAgent})  
            except ConnectionError as ce:
                print(colored('[ ERR ] archive.org connection error for page ' + page, 'red'),SPACER)
                resp = None
                return
            except Exception as e:
                print(colored('[ ERR ] Error getting response for page ' + page + ' - ' + str(e),'red'),SPACER)
                resp = None
                return
            finally:
                try:
                    if resp is not None:
                         # If a status other of 429, then stop processing Alien Vault
                        if resp.status_code == 429:
                            print(colored('[ 429 ] Archive.org rate limit reached, so stopping. Links that have already been retrieved will be saved.','red'),SPACER)
                            stopSource = True
                            return
                        # If the response from archive.org is empty then skip
                        if resp.text == '' and totalPages == 0:
                            if verbose():
                                print(colored('[ ERR ] '+url+' gave an empty response.','red'),SPACER)
                            return
                        # If a status other than 200, then stop
                        if resp.status_code != 200:
                            if verbose():
                                print(colored('[ '+str(resp.status_code)+' ] Error for '+url,'red'),SPACER)
                            return
                except:
                    pass
            
            # Get the URLs and MIME types. Each line is a separate JSON string
            for line in resp.iter_lines():
                results = line.decode("utf-8")
                try:
                    linkMimes.add(str(results).split(' ')[2])
                except Exception as e:
                    if verbose():
                        print(colored('ERROR processWayBackPage 2: Cannot get MIME type from line: ' + str(line),'red'),SPACER)
                        print(resp.text)
                try:
                    foundUrl = fixArchiveOrgUrl(str(results).split(' ')[1])
                    linksFound.add(foundUrl)
                except Exception as e:
                    if verbose():
                        print(colored('ERROR processWayBackPage 3: Cannot get link from line: ' + str(line),'red'),SPACER)         
                        print(resp.text)       
        else:
            pass
    except Exception as e:
        if verbose():
            print(colored("ERROR processWayBackPage 1: " + str(e), "red"))
    
def getWaybackUrls():
    """
    Get URLs from the Wayback Machine, archive.org
    """
    global linksFound, linkMimes, waymorePath, subs, path, stopProgram, totalPages, stopSource
    
    # Write the file of URL's for the passed domain/URL
    try:
        stopSource = False
        filterMIME = '&filter=!mimetype:warc/revisit|' + FILTER_MIME.replace(',','|') 
        filterCode = '&filter=!statuscode:' + FILTER_CODE.replace(',','|')
        
        if args.filter_responses_only:
            url = WAYBACK_URL.replace('{DOMAIN}',subs + quote(args.input) + path).replace('{COLLAPSE}','')+'&page='
        else:
            url = WAYBACK_URL.replace('{DOMAIN}',subs + quote(args.input) + path).replace('{COLLAPSE}','') + filterMIME + filterCode + '&page='
        
        # Get the number of pages (i.e. separate requests) that are going to be made to archive.org
        totalPages = 0
        try:
            print(colored('\rGetting the number of archive.org pages to search...','cyan'), end='\r')
            # Choose a random user agent string to use for any requests
            userAgent = random.choice(USER_AGENT)
            session = requests.Session()
            session.mount('https://', HTTP_ADAPTER)
            session.mount('http://', HTTP_ADAPTER)
            resp = session.get(url+'&showNumPages=True', headers={"User-Agent":userAgent}) 
            totalPages = int(resp.text.strip())
        except Exception as e:
            print(colored('[ ERR ] unable to get links from archive.org: ' + str(e), 'red'),SPACER)
            return
        
        # If the rate limit was reached end now
        if resp.status_code == 429:
            print(colored('[ 429 ] Archive.org rate limit reached so unable to get links.','red'),SPACER)
            return
        
        if verbose():
            print(colored('The archive URL requested to get links:','magenta'), url+'\n')
         
        # if the page number was found then display it, but otherwise we will just try to increment until we have everything       
        print(colored('\rGetting links from ' + str(totalPages) + ' archive.org API requests (this can take a while for some domains)...','cyan'), end='\r')

        # Get a list of all the page URLs we need to visit
        pages = set()
        for page in range(0, totalPages + 1):
            pages.add(url+str(page))

        # Process the URLs from web archive        
        if not stopProgram:
            p = mp.Pool(args.processes)
            p.map(processWayBackPage, pages)
            p.close()
            p.join()
               
        # Show the MIME types found (in case user wants to exclude more)
        if verbose() and len(linkMimes) > 0 :
            linkMimes.discard('warc/revisit')
            print(colored('MIME types found:','magenta'), str(linkMimes) + SPACER+'\n')
            linkMimes = None
        
        if not args.xcc or not args.xav:
            linkCount = len(linksFound)
            print(colored('Links found on archive.org:', 'cyan'), str(linkCount) + SPACER+'\n')
            
    except Exception as e:
        print(colored('ERROR getWaybackUrls 1: ' + str(e), 'red'))

def processCommonCrawlCollection(cdxApiUrl):
    """
    Get URLs from a given Common Crawl index collection
    """
    global subs, path, linksFound, linkMimes, stopSource
    
    try:
        if not stopSource:
            # Set mime content type filter
            filterMIME = '&filter=~mime:^(?!warc/revisit|' 
            if FILTER_MIME.strip() != '':
                filterMIME = filterMIME + FILTER_MIME.replace(',','|')
            filterMIME = filterMIME + ')'
            
            # Set status code filter
            filterCode = ''
            if FILTER_CODE.strip() != '':
                filterCode = '&filter=~status:^(?!' + FILTER_CODE.replace(',','|') + ')'
                
            commonCrawlUrl = cdxApiUrl + '?output=json&fl=timestamp,url,mime,status,digest&url=' 
               
            if args.filter_responses_only:
                url = commonCrawlUrl + subs + quote(args.input) + path
            else:
                url = commonCrawlUrl + subs + quote(args.input) + path + filterMIME + filterCode
            
            try:
                # Choose a random user agent string to use for any requests
                userAgent = random.choice(USER_AGENT)
                session = requests.Session()
                session.mount('https://', HTTP_ADAPTER)
                session.mount('http://', HTTP_ADAPTER)
                resp = session.get(url, stream=True, headers={"User-Agent":userAgent})   
            except ConnectionError as ce:
                print(colored('[ ERR ] Common Crawl connection error', 'red'),SPACER)
                resp = None
                return
            except Exception as e:
                print(colored('[ ERR ] Error getting response - ' + str(e),'red'),SPACER)
                resp = None
                return
            finally:
                try:
                    if resp is not None:
                        # If a status other of 429, then stop processing Common Crawl
                        if resp.status_code == 429:
                            print(colored('[ 429 ] Common Crawl rate limit reached, so stopping. Links that have already been retrieved will be saved.','red'),SPACER)
                            stopSource = True
                            return
                        # If the response from commoncrawl.org says nothing was found...
                        if resp.text.lower().find('no captures found') > 0:
                            # Don't output any messages, just exit function
                            return
                        # If the response from commoncrawl.org is empty, then stop
                        if resp.text == '':
                            if verbose():
                                print(colored('[ ERR ] '+url+' gave an empty response.','red'),SPACER)
                            return
                        # If a status other than 200, then stop
                        if resp.status_code != 200:
                            if verbose(): 
                                print(colored('[ '+str(resp.status_code)+' ] Error for '+url,'red'),SPACER)
                            return
                except:
                    pass
                
            # Get the URLs and MIME types
            for line in resp.iter_lines():
                results = line.decode("utf-8")
                try:
                    data = json.loads(results)
                    try:
                        linkMimes.add(data['mime'])
                    except:
                        pass
                    linksFound.add(data['url'])
                except Exception as e:
                    if verbose():
                        print(colored('ERROR processCommonCrawlCollection 2: Cannot get URL and MIME type from line: ' + str(line),'red'))
        else:
            pass
    except Exception as e:
        print(colored('ERROR processCommonCrawlCollection 1: ' + str(e), 'red'))
            
def getCommonCrawlUrls():
    """
    Get all Common Crawl index collections to get all URLs from each one
    """
    global linksFound, linkMimes, waymorePath, subs, path, stopSource
    
    try:
        stopSource = False
        linkMimes = set()
        originalLinkCount = len(linksFound)
        
        # Set mime content type filter
        filterMIME = '&filter=~mime:^(?!warc/revisit|' 
        if FILTER_MIME.strip() != '':
            filterMIME = filterMIME + FILTER_MIME.replace(',','|')
        filterMIME = filterMIME + ')'
        
        # Set status code filter
        filterCode = ''
        if FILTER_CODE.strip() != '':
            filterCode = '&filter=~status:^(?!' + FILTER_CODE.replace(',','|') + ')'
    
        if verbose():
            if args.filter_responses_only:
                url = '{CDX-API-URL}?output=json&fl=timestamp,url,mime,status,digest&url=' + subs + quote(args.input) + path
            else:
                url = '{CDX-API-URL}?output=json&fl=timestamp,url,mime,status,digest&url=' + subs + quote(args.input) + path + filterMIME + filterCode  
            print(colored('The commomcrawl index URL requested to get links (where {CDX-API-URL} is from ' + CCRAWL_INDEX_URL + '):','magenta'), url+'\n')
        
        print(colored('\rGetting commoncrawl.org index collections list...','cyan'), end='\r')
                  
        # Get all the Common Crawl index collections
        try:
            # Choose a random user agent string to use for any requests
            userAgent = random.choice(USER_AGENT)
            session = requests.Session()
            session.mount('https://', HTTP_ADAPTER)
            session.mount('http://', HTTP_ADAPTER)
            indexes = session.get(CCRAWL_INDEX_URL, headers={"User-Agent":userAgent})
        except ConnectionError as ce:
            print(colored('[ ERR ] Common Crawl connection error', 'red'),SPACER)
            return
        except Exception as e:
            print(colored('[ ERR ] Error getting Commom Crawl index collection - ' + str(e),'red'),SPACER)
            return
        
        # If the rate limit was reached end now
        if indexes.status_code == 429:
            print(colored('[ 429 ] Common Crawl rate limit reached so unable to get links.','red'),SPACER)
            return
        
        # Get the API URLs from the returned JSON
        jsonResp = indexes.json()
        cdxApiUrls = set()
        collection = 0
        for values in jsonResp:
            for key in values:
                if key == 'cdx-api':
                    cdxApiUrls.add(values[key])
            collection = collection + 1
            if collection == args.lcc: break
        
        print(colored('\rGetting links from the latest ' + str(len(cdxApiUrls)) + ' commoncrawl.org index collections (this can take a while for some domains)...','cyan'), end='\r')
             
        # Process the URLs from common crawl        
        if not stopProgram:
            p = mp.Pool(args.processes)
            p.map(processCommonCrawlCollection, cdxApiUrls)
            p.close()
            p.join()
                           
        # Show the MIME types found (in case user wants to exclude more)
        if verbose() and len(linkMimes) > 0:
            linkMimes.discard('warc/revisit')
            print(colored('MIME types found:','magenta'), str(linkMimes) + SPACER +'\n')
        
        linkCount = len(linksFound) - originalLinkCount
        print(colored('Extra links found on commoncrawl.org:', 'cyan'), str(linkCount) + SPACER+'\n')
                    
    except Exception as e:
        print(colored('ERROR getCommonCrawlUrls 1: ' + str(e), 'red'))

def processResponses():
    """
    Get archived responses from archive.org
    """
    global linksFound, subs, path, indexFile, totalResponses, stopProgram
    try:
        # Set up filters
        filterLimit = '&limit=' + str(args.limit)
        if args.from_date is None:
            filterFrom = ''
        else:
            filterFrom = '&from=' + str(args.from_date)
        if args.to_date is None:
            filterTo = ''
        else:
            filterTo = '&to=' + str(args.to_date)
        
        # Get the list again with filters and include timestamp
        linksFound = set()
        
        # Set mime content type filter
        filterMIME = '&filter=!mimetype:warc/revisit' 
        if FILTER_MIME.strip() != '':
            filterMIME = filterMIME + '|' + FILTER_MIME.replace(',','|')
            
        # Set status code filter
        filterCode = ''
        if FILTER_CODE.strip() != '':
            filterCode = '&filter=!statuscode:' + FILTER_CODE.replace(',','|')
        
        # Set the collapse parameter value in the archive.org URL. From the Wayback API docs:
        # "A new form of filtering is the option to 'collapse' results based on a field, or a substring of a field.
        # Collapsing is done on adjacent cdx lines where all captures after the first one that are duplicate are filtered out.
        # This is useful for filtering out captures that are 'too dense' or when looking for unique captures."
        if args.capture_interval == 'none': # get all
            collapse = ''
        elif args.capture_interval == 'h': # get at most 1 capture per hour
            collapse = 'timestamp:10'
        elif args.capture_interval == 'd': # get at most 1 capture per day
            collapse = 'timestamp:8'
        elif args.capture_interval == 'm': # get at most 1 capture per month
            collapse = 'timestamp:6'

        url = WAYBACK_URL.replace('{DOMAIN}',subs + quote(args.input) + path).replace('{COLLAPSE}',collapse) + filterMIME + filterCode + filterLimit + filterFrom + filterTo
            
        if verbose():
            print(colored('The archive URL requested to get responses:','magenta'), url+'\n')
        
        print(colored('\rGetting list of response links (this can take a while for some domains)...','cyan'), end='\r')
            
        # Build the list of links, concatenating timestamp and URL
        try:
            # Choose a random user agent string to use for any requests
            success = True
            userAgent = random.choice(USER_AGENT)
            session = requests.Session()
            session.mount('https://', HTTP_ADAPTER)
            session.mount('http://', HTTP_ADAPTER)
            resp = session.get(url, stream=True, headers={"User-Agent":userAgent}, timeout=args.timeout)  
        except ConnectionError as ce:
            print(colored('[ ERR ] archive.org connection error', 'red'),SPACER)
            resp = None
            success = False
            return 
        except Exception as e:
            print(colored('[ ERR ] Couldn\'t get list of responses: ' + str(e),'red'),SPACER)
            resp = None
            success = False
            return
        finally:
            try:
                if resp is not None:
                    # If the response from archive.org is empty, then no responses were found
                    if resp.text == '':
                        print(colored('No archived responses were found on archive.org for the given search parameters.','red'),SPACER)
                        success = False
                    # If a status other than 200, then stop
                    if resp.status_code != 200:
                        if verbose(): 
                            print(colored('[ '+str(resp.status_code)+' ] Error for '+url,'red'),SPACER)
                        success = False
                if not success:
                    print(colored('Failed to get links from archive.org - check input domain and try again.', 'red'),SPACER+'\n')
                    stopProgram = True
                    return
            except:
                pass
        
        # Go through the response to save the links found        
        for line in resp.iter_lines():
            try:
                results = line.decode("utf-8") 
                timestamp = results.split(' ')[0]
                originalUrl = results.split(' ')[1]
                linksFound.add(timestamp+'/'+originalUrl)
            except Exception as e:
                print(colored('ERROR processResponses 2: Cannot to get link from line: '+str(line), 'red'),SPACER)
        
        # Remove any links that have URL exclusions
        linkRequests = set()
        exclusionRegex = re.compile(r'('+re.escape(FILTER_URL).replace(',','|')+')',flags=re.IGNORECASE)
        for link in linksFound:
            # Only add the link if:
            # a) if the -ra --regex-after was passed that it matches that
            # b) it does not match the URL exclusions
            if (args.regex_after is None or re.search(args.regex_after, link, flags=re.IGNORECASE) is not None) and exclusionRegex.search(link) is None:
                linkRequests.add(link)
            
        # Get the total number of responses we will try to get
        totalResponses = len(linkRequests)
        
        # If the limit has been set over the default, give a warning that this could take a long time!
        if totalResponses > DEFAULT_LIMIT:
            print(colored('WARNING: Downloading ' + str(totalResponses) + ' responses may take a loooooooong time! Consider using arguments -l, -ci, -from and -to wisely!\n','yellow'),SPACER)

        # Create 'results' and domain directory if needed
        createDirs()
        
        # Open the index file if hash value is going to be used (not URL)
        if not args.url_filename:
            indexFile = open(
                waymorePath
                / 'results'
                / str(args.input).replace('/','-')
                / 'index.txt',
                'a',
            )
        
        # Process the URLs from web archive        
        if not stopProgram:
            p = mp.Pool(args.processes)
            p.map(processArchiveUrl, linkRequests)
            p.close()
            p.join()
            
        # Close the index file if hash value is going to be used (not URL)
        if not args.url_filename:
            indexFile.close()
        
    except Exception as e:
        print(colored('ERROR processResponses 1: ' + str(e), 'red'),SPACER)
    finally:
        linkRequests = None    

def createDirs():
    """
    Create a directory for the 'results' and the sub directory for the passed domain/URL
    """
    global waymorePath
    # Create a directory for "results" if it doesn't already exist
    try:
        results_dir = Path(waymorePath / 'results')
        results_dir.mkdir(exist_ok=True)
    except:
        pass
    # Create a directory for the target domain
    try:
        domain_dir = Path(
            waymorePath / 'results' / str(args.input).replace('/','-')
        )
        domain_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        pass     
    
# Run waymore
if __name__ == '__main__':

    # Tell Python to run the handler() function when SIGINT is received
    signal(SIGINT, handler)

    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='waymore - by @Xnl-h4ck3r: Find way more from the Wayback Machine'    
    )
    parser.add_argument(
        '-i',
        '--input',
        action='store',
        help='The target domain to find links for. This can be a domain only, or a domain with a specific path. If it is a domain only to get everything for that domain, don;t prefix with "www."',
        required=True,
        type=validateArgInput
    )
    parser.add_argument(
        '-n',
        '--no-subs',
        action='store_true',
        help='Don\'t include subdomains of the target domain (only used if input is not a domain with a specific path).',
    )
    parser.add_argument(
        '-mode',
        action='store',
        help='The mode to run: U (retrieve URLs only), R (download Responses only) or B (Both).',
        choices = ['U','R','B'],
        default='B'
    )
    parser.add_argument(
        '-f',
        '--filter-responses-only',
        action='store_true',
        help='The initial links from Wayback Machine will not be filtered (MIME Type and Response Code), only the responses that are downloaded, e.g. it maybe useful to still see all available paths from the links even if you don\'t want to check the content.',
    )
    parser.add_argument(
        '-l',
        '--limit',
        action='store',
        type=int,
        help='How many responses will be saved (if -mode is R or B). A positive value will get the first N results, a negative value will will get the last N results. A value of 0 will get ALL responses (default: '+str(DEFAULT_LIMIT)+')',
        default=DEFAULT_LIMIT,
        metavar='<signed integer>'
    )
    parser.add_argument(
        '-from',
        '--from-date',
        action='store',
        type=int,
        help='What date to get responses from. If not specified it will get from the earliest possible results. A partial value can be passed, e.g. 2016, 201805, etc.',
        metavar='<yyyyMMddhhmmss>'
    )
    parser.add_argument(
        '-to',
        '--to-date',
        action='store',
        type=int,
        help='What date to get responses to. If not specified it will get to the latest possible results. A partial value can be passed, e.g. 2016, 201805, etc.',
        metavar='<yyyyMMddhhmmss>'
    )
    parser.add_argument(
        '-ci',
        '--capture-interval',
        action='store',
        choices=['h', 'd', 'm', 'none'],
        help='Filters the search on archive.org to only get at most 1 capture per hour (h), day (d) or month (m). This filter is used for responses only. The default is \'d\' but can also be set to \'none\' to not filter anything and get all responses.',
        default='d'
    )
    parser.add_argument(
        '-ra',
        '--regex-after',
        help='RegEx for filtering purposes against links found from archive.org/commoncrawl.org AND responses downloaded. Only positive matches will be output.',
        action='store',
    )
    parser.add_argument(
        '-url-filename',
        action='store_true',
        help='Set the file name of downloaded responses to the URL that generated the response, otherwise it will be set to the hash value of the response. Using the hash value means multiple URLs that generated the same response will only result in one file being saved for that response.',
        default=False
    )
    parser.add_argument(
        '-xcc',
        action='store_true',
        help='Exclude checks to commoncrawl.org. Searching all their index collections can take a while, and it may not return any extra URLs that weren\'t already found on archive.org',
        default=False
    )
    parser.add_argument(
        '-xav',
        action='store_true',
        help='Exclude checks to alienvault.com. Searching all their pages can take a while, and it may not return any extra URLs that weren\'t already found on archive.org',
        default=False
    )
    parser.add_argument(
        '-xus',
        action='store_true',
        help='Exclude checks to urlscan.io. Searching all their pages can take a while, and it may not return any extra URLs that weren\'t already found on archive.org',
        default=False
    )
    parser.add_argument(
        '-lcc',
        action='store',
        type=int,
        help='Limit the number of Common Crawl index collections searched, e.g. \'-lcc 10\' will just search the latest 10 collections. As of June 2022 there are currently 88 collections. Setting to 0 (default) will search ALL collections. If you don\'t want to search Common Crawl at all, use the -xcc option.',
        default=0
    )
    parser.add_argument(
        '-t',
        '--timeout',
        help='This is for archived responses only! How many seconds to wait for the server to send data before giving up (default: '+str(DEFAULT_TIMEOUT)+' seconds)',
        default=DEFAULT_TIMEOUT,
        type=int,
        metavar="<seconds>",
    )
    parser.add_argument(
        '-p',
        '--processes',
        help='Basic multithreading is done when getting requests for a file of URLs. This argument determines the number of processes (threads) used (default: 3)',
        action='store',
        type=validateArgProcesses,
        default=3,
        metavar="<integer>",
    )
    parser.add_argument(
        '-r',
        '--retries',
        action='store',
        type=int,
        help='The number of retries for requests that get connection error or rate limited (default: 1).',
        default=1
    )
    parser.add_argument('-v', '--verbose', action='store_true', help="Verbose output")
    args = parser.parse_args()

    showBanner()
             
    try:     

        # Get the config settings from the config.yml file
        getConfig()

        if verbose():
            showOptions()
        
        # If the mode is U (URLs retrieved) or B (URLs retrieved AND Responses downloaded)
        if args.mode in ['U','B']:
            
            # Get URLs from the Wayback Machine (archive.org)
            if not stopProgram:
                getWaybackUrls()
        
            # If not requested to exclude, get URLs from commoncrawl.org
            if not args.xcc and not stopProgram:
                getCommonCrawlUrls()
    
            # If not requested to exclude, get URLs from alienvault.com
            if not args.xav and not stopProgram:
                getAlienVaultUrls()
            
            # If not requested to exclude, get URLs from urlscan.io
            if not args.xus and not stopProgram:
                getURLScanUrls()
                
            # Output results of all searches
            processURLOutput()
        
        # If we want to get actual archived responses from archive.org...
        if (args.mode in ['R','B'] or inputIsDomainANDPath) and not stopProgram:
            processResponses()
            
            # Output details of the responses downloaded
            processResponsesOutput()
            
    except Exception as e:
        print(colored('ERROR main 1: ' + str(e), 'red'))

    finally:
        # Clean up
        linksFound = None
        linkMimes = None
